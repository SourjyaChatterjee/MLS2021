# -*- coding: utf-8 -*-
"""HomeWork_

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1ll4PuagC31uTx2r2g7nZe9KdAvn5rnGU
"""

!git clone https://github.com/YoongiKim/CIFAR-10-images

!ls

import os
import torch
import numpy as np
import pandas as pd
from torchvision import datasets
import torchvision.transforms as transforms
from torch.utils.data.sampler import SubsetRandomSampler
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import cv2
import csv
import imageio
from PIL import Image
import matplotlib.pyplot as plt
import helper

"""**CSV Convertion of the Dataset**"""

dataset_path = "CIFAR-10-images"

train_img_path = []
test_img_path = []
train_labels =[]
test_labels = []

for dataset_type in os.listdir(dataset_path):
  if str(dataset_type) == 'train':
    for labels in os.listdir(dataset_path + '/' + str(dataset_type)):
      for files in os.listdir(dataset_path + '/' + str(dataset_type) + '/' + str(labels)):
        train_img_path.append(dataset_path + '/' + str(dataset_type) + '/' + str(labels) + '/' + str(files))
        train_labels.append(str(labels))

  if str(dataset_type) == 'test':
    for labels in os.listdir(dataset_path + '/' + str(dataset_type)):
      for files in os.listdir(dataset_path + '/' + str(dataset_type) + '/' + str(labels)):
        test_img_path.append(dataset_path + '/' + str(dataset_type) + '/' + str(labels) + '/' + str(files))
        test_labels.append(str(labels))


train_data = pd.DataFrame({ 'path' : train_img_path , 'class' : train_labels})
test_data = pd.DataFrame({ 'path' : test_img_path , 'class' : test_labels})

train_data.head()

test_data.head()

train_data = train_data.to_csv('train.csv', index = False)
test_data = test_data.to_csv('test.csv', index = False)

df = pd.read_csv('train.csv')
set(df.iloc[:,1])

i = 0
class_dict = { str(cls) : ind for ind , cls in enumerate(set(df.iloc[:,1])) }

class_dict

"""**GPU Availability**"""

# check if CUDA is available
train_on_gpu = torch.cuda.is_available()

if not train_on_gpu:
    print('CUDA is not available.  Training on CPU ...')
else:
    print('CUDA is available!  Training on GPU ...')

"""**Data Augumentation**"""

def do_your_transform(X):
  train_transforms = transforms.Compose([
                                transforms.RandomHorizontalFlip(),
                                transforms.RandomRotation(15),
                                transforms.ToTensor(),
                                transforms.Normalize((.5,.5,.5),(.5,.5,.5))])

  X_trans = train_transforms(X)
  return X_trans

"""Data loader"""

class MyDataset():
  
  def __init__(self,image_set,dict1=class_dict,argument=True):
    data = pd.read_csv(image_set)
    self.imgfiles=list(data.iloc[:,0])
    self.classlabels=list(data.iloc[:,1])
    self.argument=argument
    self.dict1=dict1

  def __len__(self):
    return len(self.imgfiles)

  def __getitem__(self,idx):
    img=cv2.imread(self.imgfiles[idx])
    X=np.asarray(img,dtype=np.uint8)
    #print(X.shape)
    if self.argument:
      pilx = Image.fromarray(img)
      X=self.dict1[do_your_transform(pilx)]
    Y=self.classlabels[idx]
    return X,Y

valid_size = 0.2

train_data = MyDataset("train.csv")
#train_data1 = MyDataset("train.csv",argument=False)
test_data = MyDataset("test.csv",argument=False)

indices = list(range(len(train_data)))
np.random.shuffle(indices)
split = int(np.floor((valid_size * len(train_data))))
valid_idx , train_idx = indices[:split] , indices[split:]

train_sampler = SubsetRandomSampler(train_idx)
valid_sampler = SubsetRandomSampler(valid_idx)

train_loader = torch.utils.data.DataLoader(train_data, batch_size = 64, sampler = train_sampler)
valid_loader = torch.utils.data.DataLoader(train_data , batch_size = 64, sampler = valid_sampler)
trainloader1 = torch.utils.data.DataLoader(train_data1, batch_size = 64)
test_loader = torch.utils.data.DataLoader(test_data, batch_size = 64, shuffle = True)

def myimshow(image, label, ax=None, title=None, normalize=True):
    """Imshow for Tensor."""
    if ax is None:
        fig, ax = plt.subplots()
    print("before : ", image.shape)
    if image.shape[2] != 3:
      image = image.numpy().transpose((1, 2, 0))
    else:
      image = image.numpy()
    print("after : ", image.shape)

    if normalize:
        mean = np.array([0.485, 0.456, 0.406])
        std = np.array([0.229, 0.224, 0.225])
        image = std * image + mean
        image = np.clip(image, 0, 1)

    ax.imshow(image)
    ax.spines['top'].set_visible(False)
    ax.spines['right'].set_visible(False)
    ax.spines['left'].set_visible(False)
    ax.spines['bottom'].set_visible(False)
    ax.tick_params(axis='both', length=0)
    ax.set_xticklabels('')
    ax.set_yticklabels('')
    ax.set_title(label)

    return ax

images, labels = next(iter(train_loader))
#print(images.shape)

fig, axes = plt.subplots(figsize=(10,4), ncols=4)
fig.suptitle("Train Data (Before Augmentation and transformation) :")
fig.tight_layout()
for ii in range(4):
    ax = axes[ii]
    myimshow(images[ii], ax=ax,label=str(labels[ii]), normalize=False)

"""**Network Architecture**"""

# define the CNN architecture
class CNN(nn.Module):
    def __init__(self):
        super().__init__()
        # convolutional layer (sees 32x32x3 image tensor)
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        # convolutional layer (sees 16x16x16 tensor)
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        # convolutional layer (sees 8x8x32 tensor)
        self.conv3 = nn.Conv2d(32, 64, 3, padding=1)
        # max pooling layer
        self.pool = nn.MaxPool2d(2, 2)
        # linear layer (64 * 4 * 4 -> 500)
        self.fc1 = nn.Linear(64 * 4 * 4, 500)
        # linear layer (500 -> 10)
        self.fc2 = nn.Linear(500, 10)
        # dropout layer (p=0.25)
        self.dropout = nn.Dropout(0.25)

    def forward(self, x):
        # add sequence of convolutional and max pooling layers
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = self.pool(F.relu(self.conv3(x)))
        # flatten image input
        x = x.view(-1, 64 * 4 * 4)
        # add dropout layer
        x = self.dropout(x)
        # add 1st hidden layer, with relu activation function
        x = F.relu(self.fc1(x))
        # add dropout layer
        x = self.dropout(x)
        # add 2nd hidden layer, with relu activation function
        x = self.fc2(x)
        return x

# create a complete CNN
model_cnn = CNN()
print(model_cnn)

# move tensors to GPU if CUDA is available
if train_on_gpu:
    model_cnn.cuda()

"""**Define Loss and Solver**"""

# specify loss function (categorical cross-entropy)
criterion = nn.CrossEntropyLoss()

# specify optimizer
optimizer = optim.SGD(model_cnn.parameters(), lr=0.01)

"""**Train With Validation**"""



def Model_train(n_epochs, model, criterion, optimizer, train_loader = train_loader, valid_loader = valid_loader):
  valid_loss_min = np.Inf # track change in validation loss

  for epoch in range(1, n_epochs+1):

    # keep track of training and validation loss
    train_loss = 0.0
    valid_loss = 0.0
    
    ###################
    # train the model #
    ###################
    model.train()
    for batch_idx, (data, target) in enumerate(train_loader):
        # move tensors to GPU if CUDA is available
        if train_on_gpu:
          data, target = data.cuda(), target.cuda()
        # clear the gradients of all optimized variables
        optimizer.zero_grad()
        # forward pass: compute predicted outputs by passing inputs to the model                
        optimizer.zero_grad()
        output = model(data)
        # calculate the batch loss
        loss = criterion(output, target)
        # backward pass: compute gradient of the loss with respect to model parameters
        loss.backward()
        # perform a single optimization step (parameter update)
        optimizer.step()
        # update training loss
        train_loss += loss.item()*data.size(0)
        
    ######################    
    # validate the model #
    ######################
    model.eval()
    for batch_idx, (data, target) in enumerate(valid_loader):
      # move tensors to GPU if CUDA is available
      if train_on_gpu:
        data, target = data.cuda(), target.cuda()
      # forward pass: compute predicted outputs by passing inputs to the model
      output = model(data)
      # calculate the batch loss
      loss = criterion(output, target)
      # update average validation loss 
      valid_loss += loss.item()*data.size(0)
    
    # calculate average losses
    train_loss = train_loss/len(train_loader.sampler)
    valid_loss = valid_loss/len(valid_loader.sampler)
        
    # print training/validation statistics 
    print('Epoch: {} \tTraining Loss: {:.6f} \tValidation Loss: {:.6f}'.format(
        epoch, train_loss, valid_loss))
    
    # save model if validation loss has decreased
    if valid_loss <= valid_loss_min:
      print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(
      valid_loss_min,
      valid_loss))
      torch.save(model.state_dict(), 'model_augmented.pt')
      valid_loss_min = valid_loss

Model_train(30 , model = model_cnn, criterion = criterion, optimizer = optimizer)



"""**Test the Model**"""

# track test loss
test_loss = 0.0
class_correct = list(0. for i in range(10))
class_total = list(0. for i in range(10))

model.eval()
# iterate over test data
for batch_idx, (data, target) in enumerate(test_loader):
    # move tensors to GPU if CUDA is available
    if train_on_gpu:
        data, target = data.cuda(), target.cuda()
    # forward pass: compute predicted outputs by passing inputs to the model
    output = model_cnn(data)
    # calculate the batch loss
    loss = criterion(output, target)
    # update test loss 
    test_loss += loss.item()*data.size(0)
    # convert output probabilities to predicted class
    _, pred = torch.max(output, 1)    
    # compare predictions to true label
    correct_tensor = pred.eq(target.data.view_as(pred))
    correct = np.squeeze(correct_tensor.numpy()) if not train_on_gpu else np.squeeze(correct_tensor.cpu().numpy())
    # calculate test accuracy for each object class
    for i in range(batch_size):
        label = target.data[i]
        class_correct[label] += correct[i].item()
        class_total[label] += 1

# average test loss
test_loss = test_loss/len(test_loader.dataset)
print('Test Loss: {:.6f}\n'.format(test_loss))

for i in range(10):
    if class_total[i] > 0:
        print('Test Accuracy of %5s: %2d%% (%2d/%2d)' % (
            classes[i], 100 * class_correct[i] / class_total[i],
            np.sum(class_correct[i]), np.sum(class_total[i])))
    else:
        print('Test Accuracy of %5s: N/A (no training examples)' % (classes[i]))

print('\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (
    100. * np.sum(class_correct) / np.sum(class_total),
    np.sum(class_correct), np.sum(class_total)))

