{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><h2>ML Assignment 1</h2></center>\n",
    "<center><h5>Sourjya Chatterjee</h5></center>\n",
    "<center>February 28, 2021</center>\n",
    "\n",
    "\n",
    "<br><br><u>**Theorem:**</u> *under Gaussian assumption linear regression amounts to least square*</b></b>\n",
    "\n",
    "<u>*proof:*</u>\n",
    "In probabilistic modelling we consider a linear model - $$y_i \\approx \\theta^Tx_i $$ Considering $\\epsilon_i $ as the random noise to model unknown effects - $$ y_i = \\theta^Tx_i + \\epsilon_i \\;\\;\\;,\\;\\;\\; \\text{where}\\;\\; \\epsilon_i \\; \\overset{\\mathrm{i.i.d}}{\\sim}\\; \\mathcal{N}(0,\\sigma^2)  $$\n",
    "The density of $\\epsilon_i$ is given by - \n",
    "\\begin{align*}\n",
    "    &p(\\epsilon_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left(-\\frac{\\epsilon_i^2}{2\\sigma^2} \\right)\\\\\n",
    "    \\Rightarrow\\;\\; & p(y_i - \\theta^Tx_i) = \\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left[-\\frac{(y_i - \\theta^Tx_i)^2}{2\\sigma^2} \\right]\n",
    "\\end{align*}\n",
    "However the conventional way to write the probablity is $$p(y_i|x_i\\;\\;;\\theta)\\;\\;=\\;\\;\\frac{1}{\\sqrt{2\\pi}\\sigma}exp\\left[-\\frac{(y_i - \\theta^Tx_i)^2}{2\\sigma^2} \\right]\\;\\;\\;\\dots(\\textbf{*})$$\n",
    "The notation $p(y_i|x_i\\;\\;;\\theta)$  indicates that this is the distribution of $y_i$ given $x_i$ and parameterized by $\\theta$\\;\\;(we can not condition on $\\theta$ since $\\theta$ is not a random variable). We can also write the distribution of $y_i$ as $(y_i|x_i\\;\\;;\\theta) \\sim\\ \\mathcal{N}(\\theta^T x_i,\\sigma^2)$. <br>\n",
    "Given X (the design matrix, which contains all the $x_i$’s) and $\\theta$, what\n",
    "is the distribution of the $y_i$’s?\n",
    "The probability of the data is given by\n",
    " $p(\\vec{y}|X;\\theta)$. This quantity is typically viewed a function of $\\vec{y}$ (and perhaps $X$),\n",
    " for a fixed value of $\\theta$. When we wish to explicitly view this as a function of $\\theta$, we will instead call it the $\\textbf{likelihood}$ function\n",
    "\\begin{align*}\n",
    " \\textbf{L}(\\theta) \n",
    " &= \\textbf{L}(\\theta;\\;\\textbf{X},\\vec{y})\\\\ \n",
    " &= p(\\vec{y}|X;\\theta) \\\\\n",
    " &= \\prod_{i=1}^{m}p(y_i|x_i;\\;\\;\\theta)\\\\\n",
    " &= \\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left[-\\frac{(y_i-\\theta^T x_i)^2}{2\\sigma^2}\\right]\n",
    "\\end{align*}\n",
    "Instead of maximizing $\\textbf{L($\\theta$)}$ , we can also maximize any strictly increasing\n",
    " function of $\\textbf{L($\\theta$)}$. In particular, the derivations will be a bit simpler if we\n",
    " instead maximize the $\\textbf{log likelihood}$\n",
    "\\begin{align*}\n",
    "  \\ell(\\theta)=\\log\\textbf{L}(\\theta)\n",
    "  &=\\log\\bigg[\\prod_{i=1}^{m}\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\left(-\\frac{(y_i-\\theta^{T} x^{(i)})^2}{2\\sigma^{2}}\\right)\\bigg] \\\\\n",
    "  &=\\sum_{i=1}^{m}\\log\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg[-\\frac{(y_i-\\theta^x_i)^2}{\n",
    "  2\\sigma^{2}}\\bigg]\\bigg)\\\\\n",
    "  &=m\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2}\\;\\;\\textbf{.}\\;\\;\\frac{1}{2}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2\n",
    "\\end{align*}  \n",
    "Hence, maximizing  gives the same answer as minimizing $\\ell(\\theta)$ $$\\frac{1}{m}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2$$\n",
    "which we recognize to be $\\textbf{J}(\\theta)$, our original least-squares cost function.<br>\n",
    "In an alternative way let the data is $\\mathcal{D} = (x_i , y_i)_{i=1}^{m}$. <br>\n",
    "Using Bayes theorem we compute $\\theta$ from data $\\mathcal{D}$ \n",
    "\\begin{align*}\n",
    "    p(\\theta|\\mathcal{D}) \n",
    "    &=\\frac{p(\\mathcal{D}|\\theta).p(\\theta)}{p(\\mathcal{D})} \\\\\n",
    "    &=\\frac{\\textbf{L}(\\theta|\\mathcal{D}).p(\\theta)}{p(\\mathcal{D})}\n",
    "\\end{align*}\n",
    "$p(\\mathcal{D}|\\theta)$ is a function of $\\theta$ given $\\mathcal{D}$ as we want to choose that particular $\\theta$ which will maximize the probability i.e. the $\\textbf{Maximum Likelihood Estimator}$.\n",
    "\\begin{align*}\n",
    "    \\theta^* &= \\underset{\\theta}{argmax}\\;\\;\\; \\textbf{L}(\\theta|\\mathcal{D})\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\;\\;  p(\\mathcal{D}|\\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax}\\;\\;\\; p(y_1,x_1,y_2,x_2,y_3,x_3,\\dots,y_m,x_m\\;\\;;\\;\\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\prod_{i=1}^{m} p(y_i,x_i\\;;\\theta) && \n",
    "    \\text{[as ($y_i,x_i$)'s are independent]}\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\prod_{i=1}^{m} p(y_i|x_i\\;;\\theta)\\textbf{.} p(x_i\\;;\\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\prod_{i=1}^{m} p(y_i|x_i\\;;\\theta)\\textbf{.} p(x_i)&&\n",
    "    \\text{[as $x_i$'s are independent of $\\theta$)]}\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\prod_{i=1}^{m} p(y_i|x_i\\;;\\theta)\\\\\n",
    "    &= \\underset{\\theta}{argmax} \\;\\sum_{i=1}^{m} log\\;[p(y_i|x_i\\;;\\theta)]\\\\\n",
    "\\end{align*}\n",
    "\\begin{align*}\n",
    "    &= \\underset{\\theta}{argmax}\\;\\sum_{i=1}^{m}\\log\\bigg(\\frac{1}{\\sqrt{2\\pi}\\sigma}\\exp\\bigg[-\\frac{(y_i-\\theta^T x_i)^2}{2\\sigma^{2}}\\bigg]\\bigg) &&\n",
    "    \\text{[from $\\textbf{*}$ we get]}\\\\\n",
    "    &= \\underset{\\theta}{argmax}\\;\\;m\\log\\frac{1}{\\sqrt{2\\pi}\\sigma}-\\frac{1}{\\sigma^2}\\;.\\;\\frac{1}{2}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2\\\\\n",
    "    &= \\underset{\\theta}{argmin} \\;\\;\\frac{1}{m}\\sum_{i=1}^{m}(y_i-\\theta^{T} x_i)^2\\\\\n",
    "\\end{align*}\n",
    "which we recognize to be $\\textbf{J}(\\theta)$, our original least-squares cost function.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
